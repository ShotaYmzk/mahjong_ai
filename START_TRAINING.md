# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹ã‚¬ã‚¤ãƒ‰

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¹ãƒ†ãƒƒãƒ—1: ç’°å¢ƒç¢ºèª
```bash
cd /home/ubuntu/Documents/mahjong_ai

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
python3 test_installation.py

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèª
python3 check_model_params.py
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
```
âœ“ All modules imported successfully
âœ“ XML parser working
âœ“ Model: 15,415,938 params (102.8% of target)
```

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿æº–å‚™
```bash
# XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’data/raw/ã«ã‚³ãƒ”ãƒ¼
# ä¾‹: Tenhouã®ã‚²ãƒ¼ãƒ ãƒ­ã‚°
cp /path/to/tenhou/logs/*.xml data/raw/

# ãƒ•ã‚¡ã‚¤ãƒ«æ•°ç¢ºèª
ls -1 data/raw/*.xml | wc -l
```

æ¨å¥¨ãƒ‡ãƒ¼ã‚¿é‡:
- **æœ€å°**: 100ã‚²ãƒ¼ãƒ  (ãƒ†ã‚¹ãƒˆç”¨)
- **æ¨å¥¨**: 10,000ã‚²ãƒ¼ãƒ 
- **ç†æƒ³**: 100,000ã‚²ãƒ¼ãƒ ä»¥ä¸Š

### ã‚¹ãƒ†ãƒƒãƒ—3: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹
```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§é–‹å§‹
bash run_train.sh

# ã¾ãŸã¯ã€Pythonã‹ã‚‰ç›´æ¥
python3 -c "
from pathlib import Path
import sys
sys.path.insert(0, str(Path.cwd()))

from src.preprocessing import MahjongDatasetBuilder
from src.model import TIT, SimplifiedDiscardOnlyHead, CompleteMahjongModel
from src.training import SupervisedTrainer, create_optimizer, create_scheduler
from src.utils import setup_logger, set_seed, get_device, load_config

# Setup
set_seed(42)
device = get_device()
logger = setup_logger('training', log_file='outputs/logs/train.log')

# Load configs
train_config = load_config('configs/train_config.yaml')
model_config = load_config('configs/model_config.yaml')

# Build dataset
builder = MahjongDatasetBuilder('data/raw', 'data/processed')
if not Path('data/processed/X.npy').exists():
    builder.build_dataset(max_games=train_config['data'].get('max_games'))

# Create dataloaders
dataloaders = builder.create_dataloaders(
    batch_size=train_config['data']['batch_size'],
    num_workers=train_config['data']['num_workers']
)

# Build model
backbone = TIT(
    input_dim=model_config['input']['input_dim'],
    d_model=model_config['tit']['d_model'],
    nhead_inner=model_config['tit']['nhead_inner'],
    nhead_outer=model_config['tit']['nhead_outer'],
    dim_feedforward=model_config['tit']['dim_feedforward'],
    dropout=model_config['tit']['dropout'],
    num_inner_layers=model_config['tit']['num_inner_layers'],
    num_outer_layers=model_config['tit']['num_outer_layers']
)

head = SimplifiedDiscardOnlyHead(
    d_model=model_config['tit']['d_model'],
    num_tiles=model_config['input']['num_tile_types'],
    dropout=model_config['tit']['dropout']
)

model = CompleteMahjongModel(backbone, head)

# Training
optimizer = create_optimizer(
    model,
    optimizer_name=train_config['training']['optimizer'],
    learning_rate=train_config['training']['learning_rate'],
    weight_decay=train_config['training']['weight_decay']
)

scheduler = create_scheduler(
    optimizer,
    scheduler_name=train_config['training']['scheduler'],
    **train_config['training']['scheduler_params']
)

trainer = SupervisedTrainer(
    model=model,
    train_loader=dataloaders['train'],
    val_loader=dataloaders['val'],
    optimizer=optimizer,
    device=device
)

print('Starting training...')
trainer.train(num_epochs=train_config['training']['num_epochs'], scheduler=scheduler)
print(f'Training completed! Best accuracy: {trainer.best_val_acc:.4f}')
"
```

## ğŸ“Š ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ç›£è¦–
```bash
# ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
tail -f outputs/logs/train.log

# å­¦ç¿’æ›²ç·šã‚’ç¢ºèª (åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«)
watch -n 10 'grep "Epoch" outputs/logs/train.log | tail -5'
```

### GPUä½¿ç”¨çŠ¶æ³
```bash
# GPUä½¿ç”¨ç‡ã‚’ç›£è¦–
watch -n 1 nvidia-smi
```

æœŸå¾…ã•ã‚Œã‚‹ä½¿ç”¨é‡:
- **VRAM**: ~2-3GB (Batch 512)
- **GPUä½¿ç”¨ç‡**: 80-95%
- **Temperature**: <80â„ƒ

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
```bash
# ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
ls -lht outputs/checkpoints/

# æœ€é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«
ls -lh outputs/checkpoints/best_acc.pth
```

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹å­¦ç¿’æ›²ç·š

### Epoch 1-20 (åˆæœŸå­¦ç¿’)
```
Epoch   Train Loss   Train Acc   Val Loss   Val Acc
-----   ----------   ---------   --------   -------
1       3.526        0.098       3.489      0.102
5       3.234        0.156       3.198      0.162
10      3.012        0.201       2.987      0.208
20      2.856        0.254       2.843      0.261
```

### Epoch 21-50 (å®‰å®šæœŸ)
```
Epoch   Train Loss   Train Acc   Val Loss   Val Acc
-----   ----------   ---------   --------   -------
30      2.745        0.289       2.738      0.294
40      2.668        0.312       2.665      0.316
50      2.612        0.329       2.611      0.331
```

### Epoch 51-100 (åæŸæœŸ)
```
Epoch   Train Loss   Train Acc   Val Loss   Val Acc
-----   ----------   ---------   --------   -------
60      2.571        0.341       2.573      0.343
80      2.538        0.352       2.545      0.349
100     2.518        0.358       2.531      0.353
```

### Epoch 101-150 (å¾®èª¿æ•´)
```
Epoch   Train Loss   Train Acc   Val Loss   Val Acc
-----   ----------   ---------   --------   -------
120     2.505        0.362       2.522      0.356
140     2.498        0.364       2.518      0.357
150     2.494        0.365       2.516      0.358
```

## ğŸ¯ è©•ä¾¡

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®è©•ä¾¡
```bash
python3 <<EOF
from src.evaluation import MetricsCalculator
from src.preprocessing import MahjongDatasetBuilder
from src.model import CompleteMahjongModel
import torch

# Load model
model = CompleteMahjongModel.load('outputs/checkpoints/best_acc.pth')
model.eval()

# Load test data
builder = MahjongDatasetBuilder('data/raw', 'data/processed')
dataloaders = builder.create_dataloaders(batch_size=256)
test_loader = dataloaders['test']

# Evaluate
metrics_calc = MetricsCalculator(num_classes=34)

with torch.no_grad():
    for inputs, targets in test_loader:
        outputs, _ = model(inputs)
        if isinstance(outputs, dict):
            outputs = outputs['discard']
        metrics_calc.update(outputs, targets)

results = metrics_calc.compute()
print("\nã€ãƒ†ã‚¹ãƒˆçµæœã€‘")
print(f"Accuracy:        {results['accuracy']:.4f}")
print(f"Top-3 Accuracy:  {results['top_3_accuracy']:.4f}")
print(f"Top-5 Accuracy:  {results['top_5_accuracy']:.4f}")
print(f"Success Prob:    {results['success_probability']:.4f}")
print(f"Hit Rate (k=5):  {results['hit_rate_top5']:.4f}")
print(f"Precision:       {results['precision_macro']:.4f}")
print(f"Recall:          {results['recall_macro']:.4f}")
print(f"F1 Score:        {results['f1_macro']:.4f}")
EOF
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ: CUDA Out of Memory
**è§£æ±ºç­–**:
```yaml
# configs/train_config.yaml
data:
  batch_size: 256  # 512ã‹ã‚‰å‰Šæ¸›
```

### å•é¡Œ: å­¦ç¿’ãŒé€²ã¾ãªã„ (Accuracy < 15%)
**è§£æ±ºç­–**:
1. Learning rateã‚’ä¸‹ã’ã‚‹: `1e-4` â†’ `5e-5`
2. Warmupè¿½åŠ ã‚’æ¤œè¨
3. ãƒ‡ãƒ¼ã‚¿ã®è³ªã‚’ç¢ºèª

### å•é¡Œ: Overfitting (Train >> Val)
**è§£æ±ºç­–**:
```yaml
# configs/model_config.yaml
tit:
  dropout: 0.2  # 0.1ã‹ã‚‰å¢—åŠ 
```

### å•é¡Œ: å­¦ç¿’ãŒé…ã„
**è§£æ±ºç­–**:
```yaml
# configs/train_config.yaml
data:
  num_workers: 8  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä¸¦åˆ—æ•°
```

```bash
# ã¾ãŸã¯ã€Mixed Precision Training
# configs/train_config.yaml
mixed_precision: true
```

## ğŸ“ ãƒ­ã‚°ã®è¦‹æ–¹

### æ­£å¸¸ãªå­¦ç¿’ã®ä¾‹
```
2025-XX-XX 10:00:00 - training - INFO - Starting training for 150 epochs
2025-XX-XX 10:00:05 - training - INFO - Epoch 1/150 - Train Loss: 3.526, Train Acc: 0.0980, Val Loss: 3.489, Val Acc: 0.1020
2025-XX-XX 10:00:10 - training - INFO - Saved best accuracy checkpoint: 0.1020
2025-XX-XX 10:02:15 - training - INFO - Epoch 2/150 - Train Loss: 3.234, Train Acc: 0.1560, Val Loss: 3.198, Val Acc: 0.1620
2025-XX-XX 10:02:20 - training - INFO - Saved best accuracy checkpoint: 0.1620
```

### å•é¡ŒãŒã‚ã‚‹å­¦ç¿’ã®ä¾‹
```
# Loss ãŒå¢—åŠ  â†’ Learning rate ãŒé«˜ã™ãã‚‹
Epoch 10/150 - Train Loss: 3.012, Val Loss: 2.987
Epoch 11/150 - Train Loss: 3.245, Val Loss: 3.156  # âš  Losså¢—åŠ 

# Accuracy ãŒåœæ» â†’ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ or ãƒ¢ãƒ‡ãƒ«å®¹é‡ä¸è¶³
Epoch 50/150 - Train Acc: 0.329, Val Acc: 0.331
Epoch 60/150 - Train Acc: 0.330, Val Acc: 0.332
Epoch 70/150 - Train Acc: 0.331, Val Acc: 0.332  # âš  åœæ»
```

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### 1. æ•™å¸«ã‚ã‚Šå­¦ç¿’å®Œäº†å¾Œ
```bash
# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦å¼·åŒ–å­¦ç¿’ã¸
# (RL trainerå®Ÿè£…å¾Œã«ä½¿ç”¨)
```

### 2. æ³¨æ„æ©Ÿæ§‹ã®å¯è¦–åŒ–
```python
from src.evaluation import AttentionVisualizer
from src.model import XAIHooks

# Attentionå¯è¦–åŒ–
xai_hooks = XAIHooks(model)
xai_hooks.register_all_attention_hooks()

# Forward pass
outputs, attention = model(test_input)
attention_weights = xai_hooks.get_attention_weights()

# å¯è¦–åŒ–
visualizer = AttentionVisualizer()
visualizer.plot_attention_heatmap(
    attention_weights['outer_transformer.layers.0'],
    save_name='attention_layer0.png'
)
```

### 3. ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤
```python
# ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
torch.save(model.state_dict(), 'final_model.pth')

# æ¨è«–ç”¨
model.eval()
with torch.no_grad():
    prediction = model(game_state)
```

---

**æº–å‚™å®Œäº†ï¼ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼** ğŸš€

